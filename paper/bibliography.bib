% Bibliography - Real Citations from Literature Review

% ========================================
% AI Agent Backdoor Attacks (Primary Related Work)
% ========================================

@article{badagent2024,
  title={BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents},
  author={Wang, Yifei and Xue, Dizhan and Zhang, Shengjie and Qian, Shengsheng},
  journal={arXiv preprint arXiv:2406.03007},
  year={2024}
}

@article{agentpoison2024,
  title={AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases},
  author={Chen, Zhaorun and Xiang, Zhen and Xiao, Chaowei and Song, Dawn and Li, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={130185--130213},
  year={2024}
}

@article{malice2024,
  title={Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain},
  author={Boisvert, Léo and Puri, Abhay and Evuru, Chandra Kiran Reddy and Chapados, Nicolas and Cappart, Quentin and Lacoste, Alexandre and Dvijotham, Krishnamurthy and Drouin, Alexandre},
  journal={arXiv preprint arXiv:2510.05159},
  year={2024}
}

@article{eia2024,
  title={EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage},
  author={Liao, Zeyi and Mo, Lingbo and Xu, Chejian and Kang, Mintong and Zhang, Jiawei and Xiao, Chaowei and Tian, Yuan and Li, Bo and Sun, Huan},
  journal={arXiv preprint arXiv:2409.11295},
  year={2024}
}

% ========================================
% Neural Network Backdoor Attacks (Foundational)
% ========================================

@inproceedings{badnets2019,
  title={BadNets: Evaluating Backdooring Attacks on Deep Neural Networks},
  author={Gu, Tianyu and Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
  booktitle={IEEE Access},
  volume={7},
  pages={47230--47244},
  year={2019}
}

@article{trojaning-llm2023,
  title={Trojaning Language Models for Fun and Profit},
  author={Zhang, Xinyang and Zhang, Zheng and Ji, Shouling and Wang, Ting},
  journal={arXiv preprint arXiv:2302.10149},
  year={2023}
}

% ========================================
% Data Poisoning Attacks
% ========================================

@inproceedings{poisoning-webscale2024,
  title={Poisoning Web-scale Training Datasets is Practical},
  author={Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
  pages={407--425},
  year={2024},
  organization={IEEE}
}

@inproceedings{poisoning-instruction2023,
  title={Poisoning Language Models During Instruction Tuning},
  author={Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
  booktitle={International Conference on Machine Learning},
  pages={35413--35425},
  year={2023},
  organization={PMLR}
}

@article{data-poisoning-llm2024,
  title={Data Poisoning in LLMs: Jailbreak-tuning and Scaling Laws},
  author={Bowen, Dillon and Murphy, Brendan and Cai, Will and Khachaturov, David and Gleave, Adam and Pelrine, Kellin},
  journal={arXiv preprint arXiv:2408.02946},
  year={2024}
}

@inproceedings{best-of-venom2024,
  title={Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data},
  author={Baumgärtner, Tim and Gao, Yang and Alon, Dana and Metzler, Donald},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{poisonbench2024,
  title={PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning},
  author={Fu, Tingchen and Sharma, Mrinank and Torr, Philip and Cohen, Shay B and Krueger, David and Barez, Fazl},
  journal={arXiv preprint arXiv:2410.08811},
  year={2024}
}

@article{temporal-poisoning2023,
  title={Temporal Robustness Against Data Poisoning},
  author={Wang, Wenxiao and Feizi, Soheil},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={47721--47734},
  year={2023}
}

% ========================================
% Backdoor Attacks in Neural Networks
% ========================================

@inproceedings{badnets2017,
  title={BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  booktitle={NIPS Workshop on Machine Learning and Computer Security},
  year={2017}
}

@article{sleeper-agents2024,
  title={Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training},
  author={Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M and Maxwell, Tim and Cheng, Newton and others},
  journal={CoRR},
  year={2024}
}

@article{backdoor-icl2023,
  title={Backdoor Attacks for In-context Learning with Language Models},
  author={Kandpal, Nikhil and Jagielski, Matthew and Tramèr, Florian and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2307.14692},
  year={2023}
}

@inproceedings{trojvlm2024,
  title={TrojVLM: Backdoor Attack Against Vision Language Models},
  author={Lyu, Weimin and Pang, Lu and Ma, Tengfei and Ling, Haibin and Chen, Chao},
  booktitle={European Conference on Computer Vision},
  pages={467--483},
  year={2024},
  organization={Springer}
}

% ========================================
% Backdoor Detection Methods
% ========================================

@inproceedings{neuralcleanse2019,
  title={Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks},
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
  booktitle={IEEE Symposium on Security and Privacy (S\&P)},
  year={2019}
}

@inproceedings{spectral2019,
  title={Spectral Signatures in Backdoor Attacks},
  author={Tran, Brandon and Li, Jerry and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{activation2019,
  title={Activation Clustering for Backdoor Detection},
  author={Chen, Bryant and Carvalho, Wilka and Baracaldo, Nathalie and Ludwig, Heiko and Edwards, Benjamin and Lee, Taesung and Molloy, Ian and Srivastava, Biplav},
  booktitle={ICML Workshop on Security and Privacy of Machine Learning},
  year={2019}
}

% ========================================
% LLM Security and Safety
% ========================================

@article{llm-security-survey2025,
  title={Security and Privacy Challenges of Large Language Models: A Survey},
  author={Das, Badhan Chandra and Amini, M Hadi and Wu, Yanzhao},
  journal={ACM Computing Surveys},
  volume={57},
  number={6},
  pages={1--39},
  year={2025}
}

@techreport{nist-adversarial2025,
  title={Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations},
  author={Vassilev, Apostol and Oprea, Alina and Fordyce, Alie and Anderson, Hyrum and Davies, Xander and Hamin, Maia},
  institution={National Institute of Standards and Technology},
  year={2025}
}

@inproceedings{safety-alignment2024,
  title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep},
  author={Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2406.05946},
  year={2024}
}

@article{no-of-course2025,
  title={No, of Course I Can! Deeper Fine-tuning Attacks that Bypass Token-level Safety Mechanisms},
  author={Kazdan, Joshua and Puri, Abhay and Schaeffer, Rylan and Yu, Lisa and Cundy, Chris and Stanley, Jason and Koyejo, Sanmi and Dvijotham, Krishnamurthy},
  journal={arXiv preprint arXiv:2502.19537},
  year={2025}
}

@inproceedings{exploitability-instruction2023,
  title={On the Exploitability of Instruction Tuning},
  author={Shu, Manli and Wang, Jiongxiao and Zhu, Chen and Geiping, Jonas and Xiao, Chaowei and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  pages={61836--61856},
  year={2023}
}

% ========================================
% Guardrails and Defense
% ========================================

@inproceedings{llamafirewall2025,
  title={LlamaFirewall: An Open Source Guardrail System for Building Secure AI Agents},
  author={Chennabasappa, Sahana and Nikolaidis, Cyrus and Song, Daniel and Molnar, David and others},
  journal={arXiv preprint arXiv:2505.03574},
  year={2025}
}

@inproceedings{granite-guardian2025,
  title={Granite Guardian: Comprehensive LLM Safeguarding},
  author={Padhi, Inkit and Nagireddy, Manish and Cornacchia, Giandomenico and others},
  booktitle={Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track)},
  pages={607--615},
  year={2025},
  organization={Association for Computational Linguistics}
}

% ========================================
% Anomaly Detection
% ========================================

@article{anomaly-survey2021,
  title={Deep Learning for Anomaly Detection: A Survey},
  author={Pang, Guansong and Shen, Chunhua and Cao, Longbing and Van Den Hengel, Anton},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--38},
  year={2021}
}

@inproceedings{systemcall-ids1996,
  title={A Sense of Self for Unix Processes},
  author={Forrest, Stephanie and Hofmeyr, Steven A and Somayaji, Anil and Longstaff, Thomas A},
  booktitle={IEEE Symposium on Security and Privacy},
  year={1996}
}

% ========================================
% AI Agent Frameworks and Systems
% ========================================

@article{agentbench2023,
  title={AgentBench: Evaluating LLMs as Agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and others},
  journal={arXiv preprint arXiv:2308.03688},
  year={2023}
}

@article{browsergym2025,
  title={The BrowserGym Ecosystem for Web Agent Research},
  author={de Chezelles, Thibault Le Sellier and Gasse, Maxime and Lacoste, Alexandre and Caccia, Massimo and Drouin, Alexandre and Boisvert, Léo and others},
  journal={Transactions on Machine Learning Research},
  year={2025}
}

@article{workarena2024,
  title={WorkArena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?},
  author={Drouin, Alexandre and Gasse, Maxime and Caccia, Massimo and Laradji, Issam H and Del Verme, Manuel and Marty, Tom and Boisvert, Léo and others},
  journal={arXiv preprint arXiv:2403.07718},
  year={2024}
}

@article{webrl2024,
  title={WebRL: Training LLM Web Agents via Self-evolving Online Curriculum Reinforcement Learning},
  author={Qi, Zehan and Liu, Xiao and Iong, Iat Long and Lai, Hanyu and Sun, Xueqiao and Sun, Jiadai and others},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2024}
}

% ========================================
% LLM Models
% ========================================

@article{llama3-2024,
  title={The Llama 3 Herd of Models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% ========================================
% Machine Learning Techniques
% ========================================

@book{statistical-learning2013,
  title={An Introduction to Statistical Learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year={2013},
  publisher={Springer}
}

@article{mann-whitney1947,
  title={On a Test of Whether One of Two Random Variables is Stochastically Larger than the Other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The Annals of Mathematical Statistics},
  pages={50--60},
  year={1947}
}

@article{cross-validation1995,
  title={A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
  author={Kohavi, Ron},
  journal={International Joint Conference on Artificial Intelligence (IJCAI)},
  volume={14},
  pages={1137--1145},
  year={1995}
}

@article{bootstrap1986,
  title={Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy},
  author={Efron, Bradley and Tibshirani, Robert},
  journal={Statistical Science},
  pages={54--75},
  year={1986}
}

% ========================================
% Feature Engineering
% ========================================

@book{feature-engineering2018,
  title={Feature Engineering for Machine Learning},
  author={Zheng, Alice and Casari, Amanda},
  year={2018},
  publisher={O'Reilly Media}
}

@article{feature-selection2003,
  title={An Introduction to Feature Selection},
  author={Guyon, Isabelle and Elisseeff, André},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={1157--1182},
  year={2003}
}

% ========================================
% Supply Chain Security
% ========================================

@article{ai-supply-chain2025,
  title={The AI Supply Chain},
  author={Gambacorta, Leonardo and Shreeti, Vatsala},
  journal={BIS Papers},
  year={2025}
}

@misc{crowdstrike2024,
  title={External Technical Root Cause Analysis — Channel File 291 Incident},
  author={CrowdStrike},
  year={2024},
  howpublished={\url{https://www.crowdstrike.com/wp-content/uploads/2024/08/Channel-File-291-Incident-Root-Cause-Analysis-08.06.2024.pdf}},
  note={Root cause analysis of the widespread IT outage on July 19, 2024}
}

@misc{cisa-solarwinds2021,
  title={Supply Chain Compromise},
  author={{Cybersecurity and Infrastructure Security Agency}},
  year={2021},
  howpublished={\url{https://www.cisa.gov/news-events/alerts/2021/01/07/supply-chain-compromise}},
  note={CISA Alert AA21-008A describing the SolarWinds Orion platform compromise (SUNBURST backdoor)}
}

@misc{cisa-xz2024,
  title={Reported Supply Chain Compromise Affecting {XZ} Utils Data Compression Library, {CVE}-2024-3094},
  author={{Cybersecurity and Infrastructure Security Agency}},
  year={2024},
  howpublished={CISA Alert, March 2024},
  note={Available: \url{https://www.cisa.gov/news-events/alerts/2024/03/29/}}
}
